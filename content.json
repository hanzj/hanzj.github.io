{"posts":[{"title":"Flink安装","text":"操作系统AlmaLinux9.1。 Flink的运行一般分为三种模式，即local、Standalone和On Yarn。因Standalone HA和on yarn模式都依赖Hadoop，所以本次只安装local和standalone。 安装java为了运行Flink，只需提前安装好 Java 11 12yum install java-11-openjdk-develjava -version Local对于 Local 模式来说，JobManager 和 TaskManager 会公用一个 JVM 来完成 Workload。如果要验证一个简单的应用，Local 模式是最方便的。实际应用中大多使用 Standalone 或者 Yarn Cluster。 123456789101112wget http://mirrors.tuna.tsinghua.edu.cn/apache/flink/flink-1.15.3/flink-1.15.3-bin-scala_2.12.tgztar -xzvf flink-1.15.3-bin-scala_2.12.tgz -C /usr/local/ln -s /usr/local/flink-1.15.3/ /usr/local/flinksed -i '/^rest.bind-address: /crest.bind-address: 0.0.0.0' /usr/local/flink/conf/flink-conf.yamlcd /usr/local/flink/bin./start-cluster.sh # 启动服务firewall-cmd --permanent --add-port=8081/tcpsystemctl reload firewalld # 停止集群命令./bin/stop-cluster.sh 通过浏览器访问http://192.168.146.129:8081 访问 Flink Dashboard，dashboard没有账号密码，如果要设置账号密码需要借助nginx或者 apache httpd 运行 demo 123/usr/local/flink/bin/flink run /usr/local/flink/examples/batch/WordCount.jar# 指定输入输出/usr/local/flink/bin/flink run /usr/local/flink/examples/batch/WordCount.jar --input input.txt --output out.txt 执行完成在 Dashboard 就可以看到相应的已经完成的Jobs。 StandaloneStandalone模式顾名思义，是在本地集群上调度执行，不依赖于外部调度机制例如YARN。 服务器规划如下，三台机器分别安装好jdk 服务器节点 用途 node129 JobManager,TaskManager node130 TaskManager node131 TaskManager 在 node129 上进行如下安装 12345678910111213141516171819202122232425262728293031323334353637wget http://mirrors.tuna.tsinghua.edu.cn/apache/flink/flink-1.15.3/flink-1.15.3-bin-scala_2.12.tgztar -xzvf flink-1.15.3-bin-scala_2.12.tgz -C /usr/local/ln -s /usr/local/flink-1.15.3/ /usr/local/flink# 修改masters配置文件，该文件用于指定主节点及其web端口，表示集群的JobManagerecho 'node129:8081' &gt; /usr/local/flink/conf/masters# 修改slaves文件，用于指定从节点，表示集群中的TaskManagerecho -e 'node129\\nnode130\\nnode131' &gt; /usr/local/flink/conf/workers# 可以查看修改后的文件内容，去除注释和空行sed '/^#/d;/^$/d' /usr/local/flink/conf/flink-conf.yamltee &gt; /usr/local/flink/conf/flink-conf.yaml &lt;&lt;EOFjobmanager.rpc.address: node129jobmanager.rpc.port: 6123jobmanager.bind-host: 0.0.0.0 jobmanager.memory.process.size: 1600mtaskmanager.bind-host: 0.0.0.0taskmanager.host: 192.168.146.129 # 所有worker节点按照实际IP地址修改taskmanager.memory.process.size: 1728mtaskmanager.numberOfTaskSlots: 2parallelism.default: 1jobmanager.execution.failover-strategy: regionrest.address: 0.0.0.0rest.bind-address: 0.0.0.0EOF# 分发flink程序scp -r /usr/local/flink-1.16.0 root@node130:/usr/localscp -r /usr/local/flink-1.16.0 root@node131:/usr/local# 添加防火墙firewall-cmd --permanent --add-port=8081/tcpfirewall-cmd --permanent --add-port=6123/tcpsystemctl reload firewalld /usr/local/flink/bin/start-cluster.sh # 启动服务 命令执行后会要求输入node129、node130和node131节点的密码，输入后自动启动三台服务器的TaskManager。启动后服务器进程如下 12345678910111213jps# node129# 14643 Jps# 14548 TaskManagerRunner# 13773 StandaloneSessionClusterEntrypoint# node130# 7457 TaskManagerRunner# 7529 Jps# node131# 5684 TaskManagerRunner# 5755 Jps 测试 1/usr/local/flink/bin/flink run /usr/local/flink/examples/batch/WordCount.jar 通过http://192.168.146.129:8081访问web界面如下，从图中可以看到有3个TaskManager，一共6个Slot 。 至此，standalone模式已成功安装。 这里只是集群模式而已，在实际场景中，我们一般需要配置为HA，防止Jobmanager突然挂掉，导致整个集群或者任务执行失败的情况发生。","link":"/posts/12027.html"},{"title":"Docker常用软件安装","text":"安装docer1234567yum install -y yum-utils device-mapper-persistent-data lvm2yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repoyum install -y docker-cesystemctl start docker# https://download.docker.com/linux/centos/7/x86_64/stable/Packages/ # 下载相应的rpm自行安装 PortainerPortainer 是Docker的轻量级，跨平台和开源管理UI。Portainer提供了Docker的详细概述，并允许您通过基于Web的简单仪表板管理容器，图像，网络和卷。它最初是Docker UI的分支。 但是，开发人员现在已经重写了几乎所有的Docker UI原始代码。 他还彻底修改了UX，并在最新版本中添加了更多功能。 截至目前，它已经引起了用户的极大关注，并且现在已经有超过一百万的下载量！ 它将支持GNU / Linux，Microsoft Windows和Mac OS X。 123456789# 下载汉化包,汉化有问题，不建议使用wget https://gitee.com/g-devops/lang-replacement/attach_files/879840/download/public-cn33.tar.gzdocker volume create portainer_datadocker run -d -p 9000:9000 --name portainer \\ --restart=always \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v portainer_data:/data \\ portainer/portainer-ce:latest Install Portainer with Docker on Linux Redis12345mkdir -p /docker_datadocker pull redisdocker run --restart=always -p 6379:6379 --name redis \\ -v /docker_data/redis.conf:/etc/redis/redis.conf -d redis redis-server /etc/redis/redis.conf zookeeper1234mkdir -p /docker_data/zookeeperdocker run -d -e TZ=&quot;Asia/Shanghai&quot; -p 2181:2181 \\ -v /docker_data/zookeeper:/data --name zookeeper \\ --restart always zookeeper MySQL12345678910111213141516mkdir -p /docker_data/mysql/conf/mysql.conf.dcat &lt;&lt;EOF &gt; /docker_data/mysql/conf/my.cnf[mysql]lower_case_table_names=1EOF-v /docker_data/mysql/conf:/etc/mysql \\docker run --name mysql57 -d -p 3306:3306 --restart=always \\-e MYSQL_ROOT_PASSWORD=root \\mysql:5.7docker exec -it mysql57 bashmysql -uroot -puse mysql; //切换数据库update user set host='%' where user='root'; //允许root用户远程访问select user,host from user; //查询flush privileges; //刷新权限立即生效 jenkins12345678910docker run -u root -d \\ -p 8080:8080 \\ -p 50000:50000 \\ -v jenkins-data:/var/jenkins_home \\ -v /var/run/docker.sock:/var/run/docker.sock \\ --name jenkins --restart=always \\ jenkinsci/blueocean# 报错 java.net.UnknownHostException: updates.jenkins.io# 进容器修改 echo nameserver 8.8.8.8 &gt; /etc/resolv.conf","link":"/posts/16107.html"},{"title":"kubeadm高可用安装k8s-基于docker","text":"环境准备软件版本 CentOS7.9.2009 docker 24.0.6 keepalived 1.3.5 haproxy 1.5.18 kubernetes 1.28.2 calico v3.26.1 dashboard 2.7.0 准备服务器这里准备了四台CentOS虚拟机，每台2核cpu和2G内，所有操作都是使用root账户。虚拟机具体信息如下表： IP地址 Hostname 节点角色 192.168.146.130 node130 control plane 192.168.146.131 node131 control plane 192.168.146.132 node132 worker nodes 192.168.146.133 node133 worker nodes 192.168.146.200 VIP 生产机集群至少需要三台 常用软件安装1yum install -y wget vim net-tools 关闭、禁用防火墙123systemctl disable firewalldsystemctl stop firewalldsystemctl status firewalld 禁用SELinux1setenforce 0 &amp;&amp; sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config 禁用swap1swapoff -a &amp;&amp; sed -i '/ swap / s/^\\\\(.*\\\\)$/#\\\\1/g' /etc/fstab 修改linux内核参数12345678910111213141516# 新增modprobe br_netfiltermodprobe overlaylsmod |grep br_netfilterlsmod |grep overlay#写入配置文件cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confvm.swappiness=0net.ipv4.ip_forward = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF#生效配置文件sysctl -p /etc/sysctl.d/k8s.conf 配置ipvs1234567891011121314yum install ipset ipvsadm -y# 添加需要加载的模块cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF#!/bin/bashmodprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrackEOF# 授权、运行、检查是否加载chmod +x /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack 安装容器环境安装docker12345678910111213141516171819202122yum install -y yum-utils device-mapper-persistent-data lvm2yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum install -y docker-ce# 镜像加速tee /etc/docker/daemon.json &lt;&lt;-'EOF'{ &quot;exec-opts&quot;:[&quot;native.cgroupdriver=systemd&quot;], &quot;registry-mirrors&quot; : [ &quot;http://ovfftd6p.mirror.aliyuncs.com&quot;, &quot;http://hub-mirror.c.163.com&quot;, &quot;https://zt8w2oro.mirror.aliyuncs.com&quot;, &quot;http://registry.docker-cn.com&quot;, &quot;http://docker.mirrors.ustc.edu.cn&quot;, &quot;https://node128&quot; ]}EOFsystemctl enable dockersystemctl start docker 配置cri-docker12345678# https://github.com/Mirantis/cri-dockerd/releaseswget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.4/cri-dockerd-0.3.4-3.el7.x86_64.rpmrpm -ivh cri-dockerd-0.3.4-3.el7.x86_64.rpmvim /usr/lib/systemd/system/cri-docker.service# ExecStart=/usr/bin/cri-dockerd --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9 --container-runtime-endpoint fd://systemctl start cri-dockersystemctl enable cri-dockercri-dockerd --version 高可用环境搭建安装haproxy12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# node130 和 node131yum install -y haproxytee /etc/haproxy/haproxy.cfg &lt;&lt;EOFglobal log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/stats#---------------------------------------------------------------------# common defaults that all the 'listen' and 'backend' sections will# use if not designated in their block#---------------------------------------------------------------------defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000#---------------------------------------------------------------------# apiserver frontend which proxys to the masters#---------------------------------------------------------------------frontend k8s-apiserver bind *:9443 mode tcp option tcplog default_backend k8s-apiserver#---------------------------------------------------------------------# round robin balancing for apiserver#---------------------------------------------------------------------backend k8s-apiserver mode tcp option tcplog option tcpcheck balance roundrobin server node131 192.168.146.131:6443 check server node130 192.168.146.130:6443 checkEOFsystemctl start haproxysystemctl enable haproxy 安装keeplived12# node130 和 node131节点yum -y install keepalived node130节点配置 1234567891011121314151617181920212223242526272829303132tee /etc/keepalived/keepalived.conf &lt;&lt;EOF! Configuration File for keepalivedglobal_defs { router_id LVS_DEVEL}vrrp_script chk_haproxy { script &quot;/bin/bash -c 'if [[ \\$(netstat -nlp | grep 9443) ]]; then exit 0; else exit 1; fi'&quot; # haproxy 检测 interval 2 # 每2秒执行一次检测 weight 11 # 权重变化}vrrp_instance VI_1 { state MASTER interface ens33 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.146.200 } track_script { chk_haproxy }}EOF node131节点配置 1234567891011121314151617181920212223242526272829tee /etc/keepalived/keepalived.conf &lt;&lt;EOF! Configuration File for keepalivedglobal_defs { router_id LVS_DEVEL}vrrp_script chk_haproxy { script &quot;/bin/bash -c 'if [[ $(netstat -nlp | grep 9443) ]]; then exit 0; else exit 1; fi'&quot; # haproxy 检测 interval 2 # 每2秒执行一次检测 weight 11 # 权重变化}vrrp_instance VI_1 { state BACKUP interface ens33 virtual_router_id 51 priority 90 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.146.200 } track_script { chk_haproxy }}EOF 启动keepalived 1234systemctl start keepalivedsystemctl enable keepalivedip a# 可以在node130上看到虚拟ip 192.168.146.200 安装 kubernetes设置kubernetes源12345678910111213# 所有节点配置cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1# 阿里云gpgcheck不过，关闭验证repo_gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFyum clean all 安装kubeadm123456789# 所有节点 安装kubelet kubeadm kubectl;kubeadm依赖kubelet和kubectlyum install -y kubelet kubeadm kubectlkubectl versionvim /etc/sysconfig/kubelet# 修改内容如下# KUBELET_EXTRA_ARGS=&quot;--cgroup-driver=systemd&quot;systemctl enable kubeletsystemctl daemon-reloadsystemctl restart kubelet 下载镜像123# 查看k8s依赖的镜像，可以在node130上逐个手动下载kubeadm config images listkubeadm config images pull --image-repository registry.aliyuncs.com/google_containers --cri-socket=unix:///var/run/cri-dockerd.sock master初始化1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# node130上执行 初始化Masterkubeadm init --kubernetes-version=1.28.2 \\--apiserver-advertise-address=192.168.146.200 \\--control-plane-endpoint &quot;192.168.146.200:6443&quot;--image-repository registry.aliyuncs.com/google_containers \\--pod-network-cidr=10.244.0.0/16 \\--cri-socket=unix:///var/run/cri-dockerd.sock# kubeadm reset &amp;&amp; rm -rf $HOME/.kube/config#################### 安装成功的提示，以下为注释 ####################Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configAlternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.confYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.146.200:6443 --token wbxz7b.6a45sappfa955f3n \\ --discovery-token-ca-cert-hash sha256:5752544368948351d9b6a7ed6cb21f53aacc34899503f17f8cd6bebb5836ecc3#################### 至此 都是注释 ##################### node131上执行 创建证书目录,安装时没有提示，确实必须的mkdir -p /etc/kubernetes/pki/etcdscp root@node130:/etc/kubernetes/admin.conf /etc/kubernetes/scp root@node130:/etc/kubernetes/pki/{ca.crt,ca.key,sa.key,sa.pub,front-proxy-ca.crt,front-proxy-ca.key} /etc/kubernetes/pki/scp root@node130:/etc/kubernetes/pki/etcd/{ca.crt,ca.key} /etc/kubernetes/pki/etcd/# node131 master加入集群kubeadm join 192.168.146.200:6443 --token wbxz7b.6a45sappfa955f3n \\ --discovery-token-ca-cert-hash sha256:5752544368948351d9b6a7ed6cb21f53aacc34899503f17f8cd6bebb5836ecc3 \\ --control-plane --cri-socket=unix:///var/run/cri-dockerd.sock# 如果是root用户，node130、node131上执行echo &quot;export KUBECONFIG=/etc/kubernetes/admin.conf&quot; &gt;&gt; ~/.bash_profile source ~/.bash_profile# 如果是非root用户，node130、node131上执行mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 第二个master加入时提示 unable to add a new control plane instance to a cluster that doesn’t have a stable controlPlaneEndpoint address Please ensure that: The cluster has a stable controlPlaneEndpoint address. The certificates that must be shared among control plane instances are provided. 解决办法： kubectl edit cm kubeadm-config -n kube-system 在 kubernetesVersion: v1.28.2 下边 添加 controlPlaneEndpoint: 192.168.146.200:6443 worker节点加入1234567# node132和node133 work 加入集群kubeadm join 192.168.146.200:6443 --token wbxz7b.6a45sappfa955f3n \\ --discovery-token-ca-cert-hash sha256:5752544368948351d9b6a7ed6cb21f53aacc34899503f17f8cd6bebb5836ecc3 \\ --cri-socket=unix:///var/run/cri-dockerd.sock# 节点查看kubectl get nodes # node节点需要较长时间才会Readykubectl get po -o wide -n kube-system 部署网络12345678910111213141516171819202122232425262728293031323334353637# 在node130上安装flannel网络kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/tigera-operator.yamlwget https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/custom-resources.yamlvim custom-resources.yaml############################ 文件内容如下# This section includes base Calico installation configuration.# For more information, see: https://projectcalico.docs.tigera.io/master/reference/installation/api#operator.tigera.io/v1.InstallationapiVersion: operator.tigera.io/v1kind: Installationmetadata: name: defaultspec: # Configures Calico networking. calicoNetwork: # Note: The ipPools section cannot be modified post-install. ipPools: - blockSize: 26 cidr: 10.244.0.0/16 # 修改此行内容 encapsulation: VXLANCrossSubnet natOutgoing: Enabled nodeSelector: all()---# This section configures the Calico API server.# For more information, see: https://projectcalico.docs.tigera.io/master/reference/installation/api#operator.tigera.io/v1.APIServerapiVersion: operator.tigera.io/v1kind: APIServermetadata: name: defaultspec: {}############################ end ############################kubectl create -f custom-resources.yamlkubectl get pods -n calico-system 查看集群状态123kubectl get pods -n calico-systemkubectl get pods -n kube-systemkubectl get node -o wide 可视化界面dashboard安装1234567891011121314151617181920212223242526272829303132333435363738wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yamlsed -i &quot;s/kubernetesui/registry.aliyuncs.com\\\\/google_containers/g&quot; recommended.yamlsed -i &quot;/targetPort: 8443/a\\\\ \\\\ \\\\ \\\\ \\\\ \\\\ nodePort: 30433\\\\n\\\\ \\\\ type: NodePort&quot; recommended.yamlkubectl apply -f recommended.yamlkubectl get all -n kubernetes-dashboard# 新增管理员账号cat &gt;&gt; dashboard-admin.yaml &lt;&lt; EOF---# ------------------- dashboard-admin ------------------- #apiVersion: v1kind: ServiceAccountmetadata: name: dashboard-admin namespace: kubernetes-dashboard---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: dashboard-adminsubjects:- kind: ServiceAccount name: dashboard-admin namespace: kubernetes-dashboardroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminEOF kubectl apply -f dashboard-admin.yamlkubectl -n kubernetes-dashboard create token dashboard-admineyJhbGciOiJSUzI1NiIsImtpZCI6Inh3cXJ2LVRHVnBuV2FSajdlOVhYbkNIOW9BLXBlQl9uejZlV3FWbDBNaDQifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNjk2NTI1MTcwLCJpYXQiOjE2OTY1MjE1NzAsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJkYXNoYm9hcmQtYWRtaW4iLCJ1aWQiOiI1OWNmZjQ3OS03NTYxLTRhMWYtYmU4OC1iZTQxZTEyM2NiZjEifX0sIm5iZiI6MTY5NjUyMTU3MCwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.qrFSZohZcUnKO_TvgXBrtgnj6F-ezyyjF8QPQjFCcQ_7gSBcMJgTXfqg5RjRibN_dNCyrg4POFzZn2RsJ76_6M8HOy-u5NZ7LOciM28aDGKMPOgZRRmzIbm9HLV40NU4mPZr7pJbL5R-nfbTzCg6sp5HKTX5GIWagSMfVscZP_wIS0SQ4FzfAnhlX2-r8rIj_PDxm47ilt2D_SOA_yO7pBz88wn4y53THwvAb8_C1g6RC837po8xpmn4KeULnPfdelXXv5YHbxWO-xU3TWHlP4iIY_Pe35KpaGXSIBpAQ-Yl6Enf2kyD1nSNUKpPMW2FK4W6yv9BIqeQVrc0O2t7FA 访问https://192.168.146.200:30433 可视化界面kuboard安装12345678910# 在 node131节点上执行docker run -d \\ --restart=unless-stopped \\ --name=kuboard \\ -p 80:80/tcp \\ -p 10081:10081/tcp \\ -e KUBOARD_ENDPOINT=&quot;http://192.168.146.131:80&quot; \\ -e KUBOARD_AGENT_SERVER_TCP_PORT=&quot;10081&quot; \\ -v /root/kuboard-data:/data \\ eipwork/kuboard:v3 通过浏览器访问 http://192.168.146.131 输入默认默认用户名：admin 密码：Kuborad123 进行登录 点击添加集群，使用token方式进行认证 如下图说明集群导入成功，点击 kuboard-admin 后进入集群概要 安装mysql进行测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 创建命名空间kubectl create ns databasetee mysql_deploy.yaml &lt;&lt;EOFapiVersion: apps/v1 # apiserver的版本kind: Deployment # 副本控制器deployment，管理pod和RSmetadata: name: mysql # deployment的名称，全局唯一 namespace: database # deployment所在的命名空间 labels: app: mysqlspec: replicas: 1 # Pod副本期待数量 selector: matchLabels: # 定义RS的标签 app: mysql # 符合目标的Pod拥有此标签 strategy: # 定义升级的策略 type: RollingUpdate # 滚动升级，逐步替换的策略 template: # 根据此模板创建Pod的副本（实例） metadata: labels: app: mysql # Pod副本的标签，对应RS的Selector spec: # nodeName: k8s-worker01 # 指定pod运行在的node containers: # Pod里容器的定义部分 - name: mysql # 容器的名称 image: mysql:8.0 # 容器对应的docker镜像 volumeMounts: # 容器内挂载点的定义部分 - name: time-zone # 容器内挂载点名称 mountPath: /etc/localtime # 容器内挂载点路径，可以是文件或目录 - name: mysql-data mountPath: /var/lib/mysql # 容器内mysql的数据目录 - name: mysql-logs mountPath: /var/log/mysql # 容器内mysql的日志目录 ports: - containerPort: 3306 # 容器暴露的端口号 env: # 写入到容器内的环境容量 - name: MYSQL_ROOT_PASSWORD # 定义了一个mysql的root密码的变量 value: &quot;123456&quot; volumes: # 本地需要挂载到容器里的数据卷定义部分 - name: time-zone # 数据卷名称，需要与容器内挂载点名称一致 hostPath: path: /etc/localtime # 挂载到容器里的路径，将localtime文件挂载到容器里，可让容器使用本地的时区 - name: mysql-data hostPath: path: /data/mysql/data # 本地存放mysql数据的目录 - name: mysql-logs hostPath: path: /data/mysql/logs # 本地存入mysql日志的目录EOFkubectl create -f mysql_deploy.yaml# 查看容器状态kubectl -n database get pods # 查看容器具体状态kubectl -n database describe pod# 命令行访问mysqlkubectl -n database exec -it mysql-5ccddd6b74-bw7qn -- mysql -uroot -p 通过kuboard登录","link":"/posts/23102.html"}],"tags":[{"name":"flink","slug":"flink","link":"/tags/flink/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"k8s","slug":"k8s","link":"/tags/k8s/"}],"categories":[{"name":"大数据","slug":"bigdata","link":"/categories/bigdata/"},{"name":"云原生","slug":"cloud","link":"/categories/cloud/"}],"pages":[]}